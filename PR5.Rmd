---
title: "Практическая работа 5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(cluster)
library(pvclust)
```

Получим и выведем набор данных mtcars.

```{r}
print(mtcars)
```

## Задание 5.1

```{r}
pairs(mtcars, main = "mtcars data", gap = 1/4)
```

Категоризованная диаграмма разбивает выборку на три группы по весу автомобиля(в тоннах, округленных до целого), показывая, что hp зависит от группирующей переменной wt. Внутри каждой группы отображается зависимость переменных hp ~
qsec, тип которой также зависит от wt.

```{r}
coplot(hp ~ qsec | as.factor(round(wt)), data = mtcars, panel = panel.smooth, rows = 1)
```

Построим также категоризованную диаграмму зависимости расхода (mpg) от мощности (hp) в группах сгруппированных по типу трансмиссии (am).

```{r}
coplot(mpg ~ hp | as.factor(am), data = mtcars, panel = panel.smooth, rows = 1)
```

##  Задание 5.2

```{r}
x<-data.frame(mtcars)
kc <- kmeans(x, 2)
aggregate(x,by=list(kc$cluster),FUN=mean)
```

Построим диаграмму для связи переменных hp и qsec.
Принадлежность к кластерам на графике отображена цветом точек, снежинками обозначены центры кластеров.

```{r}
plot(x[c("hp", "qsec")], col=kc$cluster)
points(kc$centers[,c("hp", "qsec")], col=1:3, pch=8, cex=2)
```

Построим также аналогичную диаграмму для переменных gear и drat.

```{r}
plot(x[c("gear", "drat")], col=kc$cluster)
points(kc$centers[,c("gear", "drat")], col=1:3, pch=8, cex=2)
```

## Задание 5.3

Выполним кластерный анализ методом k-средних для 3 кластеров с помощью функции kmeans() и выведем информацию о средних значениях переменных по кластерам.

```{r}
x<-data.frame(mtcars)
kc <- kmeans(x, 3)
aggregate(x,by=list(kc$cluster),FUN=mean)
```

Почти по всем переменным различие в кластерах можно увидеть в таблице выше.
Теперь покажем полученные кластеры на графике. Заштрихованные области различных цветов показывают поля
параметров объектов, относящихся к разным кластерам.

```{r}
x <- data.frame(x, kc$cluster)
clusplot(x, kc$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

## Задание 5.4

Теперь применим все изученные алгоритмы кластерного анализа для данных USArrests.

Для начала выведем наши данные и запишем их.

```{r}
x<-data.frame(USArrests)
x
```

Теперь выполним иерархическую кластеризацию с помощью функции hclust().
В качестве меры близости между объектами будем использовать манхеттенское расстояние, объединение кластеров выполним по методу Варда.

```{r}
x.pv<-pvclust(t(x),method.dist="manhattan", method.hclust="ward.D2", nboot=100)
plot(x.pv,print.num=FALSE)
```

Красным цветом на диаграмме отображаются (p-values)*100, связанные с устойчивостью кластеров в процессе репликации исходных данных. В данном примере видна хорошая устойчтвость двух больших кластеров, а также групп внутри них - значения близки к 100, что означает хорошую устойчивость.

Теперь применим метод k средних. Число кластеров определим по диаграмме каменситой осыпи.

```{r}
kl <- (nrow(x)-1)*sum(apply(x,2,var))
for (i in 2:15) kl[i] <- sum(kmeans(x, centers=i)$withinss)
plot(1:15, kl, type="b", xlab="Число кластеров", ylab="Сумма квадратов расстояний внутри кластеров")
```

Точка перелома соотвествует k = 2. Теперь выполним непосредственно сам анализ.

```{r}
kc <- kmeans(x, 2)
aggregate(x,by=list(kc$cluster),FUN=mean)
```

Построим график кластерного анализа. Заштрихованные области различных цветов как и ранее составляют поля параметров объектов, относящихся к разным кластерам.

```{r}
clusplot(x, kc$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

Теперь выполним «мягкую» кластеризацию с помощью функции fanny(). Результат выведем в виде таблицы расстояний до центров и диаграммы кластеров.

```{r}
x.f<-fanny(x,2)
head(data.frame(x.f$membership))
plot(x.f, which=1)
```

